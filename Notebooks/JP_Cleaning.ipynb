{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Importando librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Análisis exploratorio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Datasets/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(df):                        # Funcion para explorar los elementos del dataset\n",
    "                                         # Ayudará para encontrar valores nulos\n",
    "    columns=df.columns.to_list()         # Además ayudará a encontrar columnas object y columnas int\n",
    "    ncol=df.describe().columns.to_list()\n",
    "    ccol=[]\n",
    "    for i in columns:\n",
    "        if(ncol.count(i)==0):\n",
    "            ccol.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    print('Name of all columns in the dataframe:')\n",
    "    print(columns)\n",
    "    print('')\n",
    "    print('Number of columns in the dataframe:')\n",
    "    print(len(columns))\n",
    "    print('')\n",
    "    print('Name of all numerical columns in the dataframe:')\n",
    "    print(ncol)\n",
    "    print('')\n",
    "    print('Number of numerical columns in the dataframe:')\n",
    "    print(len(ncol))\n",
    "    print('')\n",
    "    print('Name of all categorical columns in the dataframe:')\n",
    "    print(ccol)\n",
    "    print('')\n",
    "    print('Number of categorical columns in the dataframe:')\n",
    "    print(len(ccol))\n",
    "    print('')\n",
    "    print('------------------------------------------------------------------------------------------------')\n",
    "    print('')\n",
    "    print('Number of Null Values in Each Column:')\n",
    "    print('')\n",
    "    print(df.isnull().sum())\n",
    "    print('')\n",
    "    print('')\n",
    "    print('Number of Unique Values in Each Column:')\n",
    "    print('')\n",
    "    print(df.nunique())\n",
    "    print('')\n",
    "    print('')\n",
    "    print('Basic Statistics and Measures for Numerical Columns:')\n",
    "    print('')\n",
    "    print(df.describe().T)\n",
    "    print('')\n",
    "    print('')\n",
    "    print('Other Relevant Metadata Regarding the Dataframe:')\n",
    "    print('')\n",
    "    print(df.info())\n",
    "    print('')\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe=['g','r']\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(3,2,1)\n",
    "plt.style.use('seaborn')\n",
    "plt.tight_layout()\n",
    "sns.set_context('talk')\n",
    "sns.histplot(data=df, x=\"Sex\", hue=\"HeartDisease\",multiple=\"stack\",palette=oe)\n",
    "#ax.set(xlabel='Sex ', ylabel='Count')\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "plt.style.use('seaborn')\n",
    "plt.tight_layout()\n",
    "sns.set_context('talk')\n",
    "sns.histplot(data=df, x=\"ChestPainType\", hue=\"HeartDisease\",multiple=\"stack\",palette=oe)\n",
    "#ax.set(xlabel='ChestPainType', ylabel='Count')\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "plt.style.use('seaborn')\n",
    "plt.tight_layout()\n",
    "sns.set_context('talk')\n",
    "sns.histplot(data=df, x=\"ExerciseAngina\", hue=\"HeartDisease\",multiple=\"stack\",palette=oe)\n",
    "#ax.set(xlabel='ExerciseAngina', ylabel='Count')\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "plt.style.use('seaborn')\n",
    "plt.tight_layout()\n",
    "sns.set_context('talk')\n",
    "sns.histplot(data=df, x=\"RestingECG\", hue=\"HeartDisease\",multiple=\"stack\",palette=oe)\n",
    "#ax.set(xlabel='RestingECG', ylabel='Count')\n",
    "\n",
    "plt.subplot(3,2,5)\n",
    "plt.style.use('seaborn')\n",
    "plt.tight_layout()\n",
    "sns.set_context('talk')\n",
    "sns.histplot(data=df, x=\"ST_Slope\", hue=\"HeartDisease\",multiple=\"stack\",palette=oe)\n",
    "#ax.set(xlabel='ST_Slope', ylabel='Count')\n",
    "\n",
    "plt.subplot(3,2,6)\n",
    "plt.style.use('seaborn')\n",
    "plt.tight_layout()\n",
    "sns.set_context('talk')\n",
    "sns.histplot(data=df, x=\"FastingBS\", hue=\"HeartDisease\",multiple=\"stack\",palette=oe)\n",
    "#ax.set(xlabel='FastingBS', ylabel='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para visualizar ouliers\n",
    "\n",
    "def outliers(df_column):\n",
    "    q75, q25 = np.percentile(df_column, [75 ,25]) \n",
    "    iqr = q75 - q25\n",
    "    print('q75: ',q75)\n",
    "    print('q25: ',q25)\n",
    "    print('Inter Quartile Range: ',iqr)\n",
    "    print('Outliers lie before', q25-1.8*iqr, 'and beyond', q75+1.8*iqr) \n",
    "\n",
    "  # Normalmente se usa un IQR de 1.5, pero he usado 1.8 para tener un rango más amplio\n",
    "\n",
    "    print('Number of Rows with Left Extreme Outliers:', len(df[df_column <q25-1.8*iqr]))\n",
    "    print('Number of Rows with Right Extreme Outliers:', len(df[df_column>q75+1.8*iqr]))\n",
    "    plt.tight_layout()\n",
    "    plt.style.use('seaborn')\n",
    "    sns.set_context('notebook')\n",
    "    sns.histplot(data=df, x=df_column, hue=\"HeartDisease\",multiple=\"stack\",palette=oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers(df['RestingBP']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como los outliers estan ligeramente sesgados hacia el extremo derecho, me los quedo y dropeo los del extremo izquierdo\n",
    "df=df[df.RestingBP>=84]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers(df['Cholesterol']) \n",
    "#Es imposible que una persona tenga 0 de colesterol xdddd\n",
    "#Más adelante lidiaré con este problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#De momento me quedo con los valores menores de 500 de colesterol\n",
    "df=df[df.Cholesterol<=500]\n",
    "df.head()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers(df['MaxHR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers(df['Oldpeak'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Cambiando las cadenas de texto con valores numéricos usando One Hit Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OHE(dfcolumn):\n",
    "    global df\n",
    "    dfcolumn.nunique()\n",
    "    len(df.columns)\n",
    "    finallencol = (dfcolumn.nunique() - 1) + (len(df.columns)-1)\n",
    "    dummies = pd.get_dummies(dfcolumn, drop_first=True, prefix=dfcolumn.name)\n",
    "    df=pd.concat([df,dummies],axis='columns')\n",
    "    df.drop(columns=dfcolumn.name,axis=1,inplace=True) #Me cargo la columna original\n",
    "    if(finallencol==len(df.columns)):\n",
    "        print('Todo correcto') \n",
    "        print('')\n",
    "    else:\n",
    "        print('Error')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHE(df['ChestPainType'])\n",
    "OHE(df['Sex'])\n",
    "OHE(df['RestingECG'])\n",
    "OHE(df['ExerciseAngina'])\n",
    "OHE(df['ST_Slope'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Lidiando con el problema de la gente que tiene el colesterol a 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Cholesterol==0].mean() #Intento de entender cómo se ven los datos cuando hay valores missing, como el colesterol a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Cholesterol>0].mean() #Aqui viendo si los que tienen el colesterol a 0 no se tienen en cuenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: ',df['Cholesterol'].mean())\n",
    "print('Median: ',df['Cholesterol'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Cholesterol']>0].Cholesterol.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relleno los valores 0 del colesterol usando KNN-Imputation \n",
    "df['Cholesterol'].replace(to_replace = 0, value =np.nan, inplace=True)\n",
    "KNN_imputed = KNNImputer(n_neighbors=5)\n",
    "I=KNN_imputed.fit_transform(df)\n",
    "Cholesterol=[]\n",
    "for i in range(0,len(df)):\n",
    "  Cholesterol.append(I[i][2])\n",
    "df['Cholesterol']=Cholesterol "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outliers(df['Cholesterol']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Transformando los datos para tener mejores resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF (Variance Inflation Factor) para buscar colinealidad entre variables independientes\n",
    "vif = df.copy()\n",
    "vif.drop(columns='HeartDisease',axis=1,inplace=True)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(vif.values, i) for i in range(len(vif.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data # Una buena práctica es trabajar con variables que tengan un VIF >= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalando los datos para mayor eficacia\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # #Convirte a standard normal distribtuion\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(vif)\n",
    "scaled_data=scaler.transform(vif)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA # Reduciendo dimensiones usando Principal Component Analysis para compensar las variables que tengan un VIF alto\n",
    "pca=PCA(n_components=11)\n",
    "pca.fit(scaled_data)\n",
    "x_pca=pca.transform(scaled_data)\n",
    "x_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"HeartDisease\"] = scaler.fit_transform(df[\"HeartDisease\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1 Haciendo la X y la Y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"HeartDisease\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns='HeartDisease',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Modelo final y algoritmos usados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='rbf', random_state=0)\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('Confusion Matrix')\n",
    "print('------------------------')\n",
    "print('')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "plot_confusion_matrix(classifier, x_test, y_test,cmap=\"binary\") \n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 Logistic Regression Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train, y_train)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(x_test, y_test)))\n",
    "print('')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 103, stop = 300, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_pca, df['HeartDisease'], test_size=0.25, random_state=0)\n",
    "clf=RandomForestClassifier(n_estimators=124,min_samples_split= 2,\n",
    "                           min_samples_leaf= 1,max_features='sqrt',max_depth=None, bootstrap=False)\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred=clf.predict(x_test)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('Confusion Matrix')\n",
    "print('------------------------')\n",
    "print('')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "plot_confusion_matrix(clf, x_test, y_test,cmap=\"binary\") \n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.4 Support Vector Machine (Radial Basis Function Kernel) with Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
    "            'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "            'kernel': ['rbf']}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3, cv=5)\n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "y_pred = grid.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('Confusion Matrix')\n",
    "print('------------------------')\n",
    "print('')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "plot_confusion_matrix(grid, x_test, y_test,cmap=\"binary\") \n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.5 KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_pca, df['HeartDisease'], test_size=0.25, random_state=0)\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred=knn.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('Confusion Matrix')\n",
    "print('------------------------')\n",
    "print('')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "plot_confusion_matrix(knn, x_test, y_test,cmap=\"binary\") \n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.6 Gradient Boosting Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "clff = GradientBoostingClassifier(n_estimators=100, learning_rate=0.2, max_depth=1, random_state=23)\n",
    "clff.fit(x_train, y_train)\n",
    "y_pred=clff.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('Confusion Matrix')\n",
    "print('------------------------')\n",
    "print('')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "plot_confusion_matrix(clff, x_test, y_test,cmap=\"binary\") \n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## He intentado tb usar Neural Network Using Tensor flow y el Gaussian Navie Bayes Algorithm pero eran demasiado complicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'modelo_final2.sav'\n",
    "pickle.dump(clff, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.loc[750]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df==df.loc[750]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = df.loc[491]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df==df.loc[491]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clff.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
